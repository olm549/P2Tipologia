---
title: 'Tipología y ciclo de vida : PRA2 - Selección y preparación de un juego de datos'
author: "Autores: Óscar López Montero y Jose Antonio Jara"
date: "Abril 2021"
output:
  pdf_document:
    latex_engine: xelatex
    highlight: default
    number_sections: yes
    toc: yes
    toc_depth: '4'
  word_document: default
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 4
    includes:
      in_header: PEC-header.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
******
# Introducción
******
## Presentación

En esta práctica se elabora un caso práctico orientado a aprender a identificar los datos
relevantes para un proyecto analítico y usar las herramientas de integración, limpieza, validación
y análisis de las mismas.

Para hacer esta práctica tendréis que trabajar en grupos de 2 personas.
Tendréis que entregar un solo archivo con el enlace Github (https://github.com) donde se
encuentren las soluciones incluyendo los nombres de los componentes del equipo. 

Podéis utilizar la Wiki de Github para describir vuestro equipo y los diferentes archivos que corresponden a
vuestra entrega. Cada miembro del equipo tendrá que contribuir con su usuario Github.Aunque no se trata del mismo enunciado, los siguientes ejemplos de ediciones anteriores os pueden servir como guía:

## Competencias

En esta práctica se desarrollan las siguientes competencias del Máster de Data Science:

● Capacidad de analizar un problema en el nivel de abstracción adecuado a cada situación y aplicar las habilidades y conocimientos adquiridos para abordarlo y resolverlo.

● Capacidad para aplicar las técnicas específicas de tratamiento de datos (integración, transformación, limpieza y validación) para su posterior análisis.

## Objetivos

Los objetivos concretos de esta práctica son:

● Aprender a aplicar los conocimientos adquiridos y su capacidad de resolución de problemas en entornos nuevos o poco conocidos dentro de contextos más amplios o multidisciplinares.

● Saber identificar los datos relevantes y los tratamientos necesarios (integración, limpieza y validación) para llevar a cabo un proyecto analítico.

● Aprender a analizar los datos adecuadamente para abordar la información contenida en los datos.

● Identificar la mejor representación de los resultados para aportar conclusiones sobre el problema planteado en el proceso analítico.

● Actuar con los principios éticos y legales relacionados con la manipulación de datos en función del ámbito de aplicación.

● Desarrollar las habilidades de aprendizaje que les permitan continuar estudiando de un modo que tendrá que ser en gran medida autodirigido o autónomo.

● Desarrollar la capacidad de búsqueda, gestión y uso de información y recursos en el ámbito de la ciencia de datos. 


## Descripción de la Práctica a realizar


El objetivo de esta actividad será el tratamiento de un dataset, que puede ser el creado en la
práctica 1 o bien cualquier dataset libre disponible en Kaggle (https://www.kaggle.com).

Algunos ejemplos de dataset con los que podéis trabajar son:

● Red Wine Quality (https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)

● Titanic: Machine Learning from Disaster (https://www.kaggle.com/c/titanic)

El último ejemplo corresponde a una competición activa de Kaggle de manera que, opcionalmente, podéis aprovechar el trabajo realizado durante la práctica para entrar en esta competición.

Siguiendo las principales etapas de un proyecto analítico, las diferentes tareas a realizar (y justificar) son las siguientes:

1. Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?

2. Integración y selección de los datos de interés a analizar.

3. Limpieza de los datos.

      3.1. ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos?
  
      3.2. Identificación y tratamiento de valores extremos.
  
4. Análisis de los datos.

     4.1. Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).
  
     4.2. Comprobación de la normalidad y homogeneidad de la varianza.
  
     4.3. Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo        del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc. Aplicar al menos tres       métodos de análisis diferentes.
  
5. Representación de los resultados a partir de tablas y gráficas.

6. Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?

7. Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar en Python.

## Recursos Básicos

Los siguientes recursos son de utilidad para la realización de la práctica:

● Calvo M., Subirats L., Pérez D. (2019). Introducción a la limpieza y análisis de los datos. Editorial UOC.

● Megan Squire (2015). Clean Data. Packt Publishing Ltd.

● Jiawei Han, Micheine Kamber, Jian Pei (2012). Data mining: concepts and techniques. Morgan Kaufmann.

● Jason W. Osborne (2010). Data Cleaning Basics: Best Practices in Dealing with Extreme Scores. Newborn and Infant Nursing Reviews; 10 (1): pp. 1527-3369.

● Peter Dalgaard (2008). Introductory statistics with R. Springer Science & Business Media.

● Wes McKinney (2012). Python for Data Analysis. O’Reilley Media, Inc.

● Tutorial de Github https://guides.github.com/activities/hello-world



## Criterios de valoración

Todos los apartados son obligatorios. La ponderación de los ejercicios es la siguiente:

● Los apartados 1, 2 y 6 valen 0,5 puntos.

● Los apartados 3, 5 y 7 valen 2 puntos.

● El apartado 4 vale 2,5 puntos.

Se valorará la idoneidad de las respuestas, que deberán ser claras y completas. Las diferentes etapas deberán justificarse y acompañarse del código correspondiente. También se valorará la síntesis y claridad, a través del uso de comentarios, del código resultante, así como la calidad de los datos finales analizados.

**Ejercicios prácticos** 

Para todas las PRA es **necesario documentar** en cada apartado del ejercicio práctico que se ha hecho y como se ha hecho.

## Formato y fecha de entrega PRA_1

Durante la semana del 24 al 28 de mayo el grupo podrá entregar al profesor una entrega parcial opcional. Esta entrega parcial es muy recomendable para recibir asesoramiento sobre la práctica y verificar que la dirección tomada es la correcta. Se entregarán comentarios a los estudiantes que hayan efectuado la entrega parcial pero no contará para la nota de la práctica. En la entrega parcial los estudiantes deberán entregar por correo electrónico, al profesor encargado del aula, el enlace al repositorio Github con el que hayan avanzado.

En referente a la entrega final, hay que entregar un único fichero que contenga el enlace Github, el cual no se podrá modificar posteriormente a la fecha de entrega, donde haya:

1. Una Wiki con los nombres de los componentes del grupo y una descripción de los
ficheros.
2. Un documento PDF con las respuestas a las preguntas y los nombres de los componentes del grupo. Además, al final del documento, deberá aparecer la siguiente tabla de contribuciones al trabajo, la cual debe firmar cada integrante del grupo con sus iniciales.
Las iniciales representan la confirmación de que el integrante ha participado en dicho apartado. Todos los integrantes deben participar en cada apartado, por lo que, idealmente, los apartados deberían estar firmados por todos los integrantes..
3. Una carpeta con el código generado para analizar los datos.
4. El fichero CSV con los datos originales.
5. El fichero CSV con los datos finales analizados.
Este documento de entrega final de la Práctica 2 se debe entregar en el espacio de Entrega y Registro de AC del aula antes de las 23:59 del día 8 de junio. No se aceptarán entregas fuera de plazo.

## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar donde se obtuvo y su estatus legal: si la obra esta protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra esta protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Resolución de la práctica
******

En esta práctica, hemos decidido trabajar sobre el dataset Heart Attack Analysis & Prediction Dataset situado en el repositorio de Kaggle.com. El dataset contiene diferentes muestras sobre pacientes susceptibles a recibir ataques al corazón.

******
## Descripción del Dataset
******

El dataset sobre el que vamos a realizar el análisis y predicción trata sobre enfermedades cardiovasculares, en concreto, de la susceptibilidad de algunas personas a recibir ataques al corazón.

El objetivo de la práctica es analizar un conjunto de datos con el fin de identificar que tipo de factores son los más influyentes sobre los pacientes que han recibido ataques al corazón, y con ello, poder predecir con mayor facilidad cuando es más probable que ocurra y qué tipo de personas sufren de un mayor riesgo.

Actualmente en España existen más de 10 millones de personas con enfermedades y/o patologías relacionadas con el corazón, y más de 120.000 fallecen como consecuencia de estas. Entre todas estás personas, sólo en España, más de 14.000 personas fallecieron anualmente a causa de un infarto agudo de miocardio.

La predicción en temas de salud es de gran ayuda a la hora de tratar a algunos pacientes con el fin de que el diagnóstico de su médico sea lo más preciso posible, poder predecir qué personas están más expuestas a este tipo de problemas cardiovasculares, así como los principales causantes puede ayudar a más de un médico a la hora de recomendar un tratamiento a su cliente, llegando a salvar más de una vida.

Antes de explicar las variables que vamos a utilizar en nuestro dataset, y que vamos a analizar con el fin de determinar su incidencia en los infartos agudos de miocardio, vamos a explicar de qué tratan, así como algunas de las pruebas que los médicos realizan a sus pacientes, con el fin de entender los procedimientos y estudios que son realizados hoy en día.

Un infarto agudo de miocardio o ataque al corazón es provocado por la muerte de células cardiacas debido al desequilibrio entre el aporte de riego sanguineo por la circulación coronaria y la demanda del mismo. Este suministro deficiente de oxígeno al corazón es el resultante de anginas de pecho, las cuales, suelen preceder a los infartos si no es tratada, ya que acaban produciendo la muerte celular explicada anteriormente.

Las variables que componen nuestro dataset y que vamos a utilizar para estudiar estos casos son: 

* age : Edad del paciente.

* sex : Género del paciente.

* cp : Tipo de dolor en el pecho

  0. Asintomático.
  1. Angina típica.
  2. Angina atípica.
  2. Dolor no-anginal.
  
* trtbps : Presión arterial en reposo (en mm Hg)

* chol : Colesterol (en mg/dl)

* fbs : (Glucemia > 120 mg/dl) (1 = True; 0 = False)

* restecg : Resultados de electrocardiograma en reposo.

  0. Muestra una probable hipertrófia del ventrículo izquierdo.
  1. Anomalías de onda ST-T (Inversiones de onda T y/o una elevación de la onda en el segmento ST > 0.05 mV)
  2. Normal

* thalachh : Ritmo cardiaco máximo.

* exng: Si el paciente ha sufrido una angina provocada por el ejercicio (1 = Si, 0 = No)

* oldpeak: Depresión en el segmento ST al hacer ejercicio en relación con el reposo.

* slp: Pendiente del segmento ST en la electrocardiograma.

  0. Pendiente descendente.
  1. Plano.
  2. Pendiente ascendente.
  
  
* caa: Número de vasos principales del corazón (De 0 a 3).

* thall: Prueba de esfuerzo cardiaco (Thallium stress test).

  1. El test muestra un defecto irreversible.
  2. El flujo sanguineo se encuentra dentro de los valores normales.
  3. El test muestra un defecto reversible.


* target : Variable de clase. 0 equivale a un menor riesgo de ataque cardiaco mientras que 1 equivale un mayor riesgo.

Tras las variables, vamos a explicar de una manera más informada el significado de las mismas.

En cada latido del corazón, el impulso cardiaco sucede en la nódulo sinusal (Situado en la aurícula derecha), para después diseminarse por las aurículas, produciendo la despolarización y por consiguiente, la contracción de las mismas. Tras esto, llega al nódulo aurioventricular, situada en la parte izquierda de la aurícula derecha, donde la onda eléctrica sufre una breve pausa de 100ms. Tras esta pausa, la onda eléctrica se disemina a través del *haz de His* (Un fino cordón muscular) llegando a los ventrículos y despolarizándolos y ocasionando la contracción ventricular.

Un electrocardiograma es una representación visual de la actividad eléctrica del corazón en función del tiempo.
Este, consta principalmente de 5 fases:

1. Onda P: La primera ligera curva hacia arriba que aparece en el electrocardiograma (ECG). Es el momento en el que las aurículas se contraen y envian sangre hacia los ventrículos.

2. Segmento P-R: Periodo entre la Onda P y la siguiente deflexión o curva, en este las aurículas están terminando de vaciarse.

3. Complejo QRS: Periodo en el que los ventrículos se cotraen, expulsando su contenido. Este complejo está compuesto por las ondas Q,R y S. La onda Q no siempre aparece en el ECG, pero se caracteriza por ser la primera pequeña deflexión negativa de este complejo. Esta es seguida por la onda R, que varia en altura dependiendo de las condiciones físicas del paciente (A mayores capacidades físicas, mayor altura). La onda S es la continuación de la onda R, y se caracteriza por ser la fase decreciente del complejo.

4. Segmento ST: Es la fase que más vamos a estudiar en este caso. Se trata del trazado lineal entre la onda S y la onda T. Su pendiente en relación con la linea basal puede llegar a significar insuficiencia de riego cardiaco. Elevaciones y depresiones en este segmento superiores a 1 mm pueden llegar a significar la oclusión de una arteria coronaria.

5. Onda T: También utilizada en nuestras variables. La onda T consiste en una pequeña deflexión positiva que refleja la repolarización ventricular o el momento en el que el corazón se encuentra en estado de relajación tras expulsar la sangre que se hallaba en los ventrículos. En los casos en los que esta onda está invertida, si la inversión es superior a 1mm suele asociarse a casos de isquemia miocárdica.


Por otro lado, vamos a explicar también de qué trata el Thallium stress test o prueba de esfuerzo cardiaco.

Esta prueba es realizada con el fin de mostrar cómo fluye la sangre en el corazón mientras se hace ejercicio o se está en reposo, para ver la capacidad del corazón para responder al estrés físico en un entorno clínico controlado comparando la circulación coronaria en reposo con la obtenida mientras se está en el momento de esfuerzo cardiaco cumbre.

Para realizar el test, se introduce un líquido con una pequeña cantidad de radioactividad llamado radioisótopo en el flujo sanguineo, este fluirá por tu sangre hasta llegar al corazón para más tarde poder ser inspeccionado por una cámara gamma con el fin de detectar discrepancias en el funcionamiento del músculo.


******
## Integración y selección de los datos de interés
******

Para esta práctica vamos a utilizar todos los datos que nos otorga el dataset, pese a tener distinta influencia a la hora de que un paciente sea considerado de riesgo.

Leemos y revisamos los datos:

```{r echo=TRUE, message=FALSE, warning=FALSE}
heartdata<-read.csv('../data/heart.csv')
summary(heartdata)
str(heartdata)
```

Podemos ver como las variables categóricas aún están representadas por valores numéricos, vamos a transformarlas con el fin de hacer el análisis más visual y entendible.

```{r echo=TRUE, message=FALSE, warning=FALSE}

heartdata$output <- as.factor(heartdata$output)
levels(heartdata$output) <- c("Bajo riesgo", "Alto riesgo")
summary(heartdata$output)
heartdata$sex <- as.factor(heartdata$sex)
levels(heartdata$sex) <- c("Mujer", "Hombre")
summary(heartdata$cp)
heartdata$cp <- as.factor(heartdata$cp)
levels(heartdata$cp) <- c("Asintomático","Angina típica","Angina atípica", 
                          "Dolor no anginal")
heartdata$fbs <- ifelse(heartdata$fbs ==1, T,F)
#heartdata$fbs <- as.logical(heartdata$fbs)
heartdata$fbs <- as.factor(heartdata$fbs)
summary(heartdata$fbs)
heartdata$restecg <- as.factor(heartdata$restecg)
levels(heartdata$restecg) <- c("Normal","Anomalía de onda ST-T","Hipertrofia")
heartdata$exng <- ifelse(heartdata$exng ==1, T,F)
#heartdata$exng <- as.logical(heartdata$exng)
heartdata$exng <- as.factor(heartdata$exng)
heartdata$slp <- as.factor(heartdata$slp)
levels(heartdata$slp) <- c("Pendiente ascendente","Plano",
                           "Pendiente descendente")
heartdata$thall <- as.factor(heartdata$thall)
summary(heartdata$thall)
```

En el caso de la prueba de esfuerzo cardiaco, tenemos variables nulas mapeadas a 0, dado que no tenemos este dato, que son únicamente dos registros y que no podemos estimarlos mediante modelos lineales, vamos a eliminar estos dos casos. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
heartdata <- heartdata[heartdata$thall!=0,]
heartdata$thall<-droplevels(heartdata$thall)
levels(heartdata$thall) <- c("Defecto irreversible","Normal",
                             "Defecto reversible")
summary(heartdata$thall)
```

******
## Limpieza de los datos
******

### Tratamiento de elementos vacíos

Procedemos a comprobar y eliminar/reemplazar los valores nulos de nuestro dataset. Aunque no parecen contener campos vacíos tras la primera toma de contacto y haberlos revisado previamente en Kaggle.

```{r echo=TRUE, message=FALSE, warning=FALSE}
colSums(is.na(heartdata))
```

Vemos como no tenemos ningún valor vacío en nuestro conjunto de datos. En el caso de encontrarnos un valor vacío en una variable categórica, y que esta sea importante para el análisis se eliminaría. Por otro lado, si tenemos un valor vacío en una variable cuantitativa, podriamos estimarlo a traves de un modelo de regresión junto a otra variable cuantitativa con la que mantenga buena correlación.

Tras comprobarlo, guardamos los datos tras ser tratados en un nuevo fichero.

```{r echo=TRUE, message=FALSE, warning=FALSE}
write.csv(heartdata, "../data/heart_clean.csv")
```

### Identificación y tratamiento de valores extremos

Vamos a realizar diagramas de caja y bigotes sobre las variables cuantitativas con el fin de encontrar outliers o valores atípicos.

Unimos los datos cuantitativos usando la función melt de la librería reshape.

Y mostramos los diagramas usando ggplot.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(reshape2)
library(ggplot2)
hd.m <- melt(heartdata[c(1,4,5,8,10)], id.var = NULL)
gbp <-ggplot(hd.m,aes(x=variable,y=value, fill=variable)) + geom_boxplot()
gbp + facet_wrap( ~ variable, scales="free")
```

Podemos observar tal vez un outlier que destaca en la variable que mide la cantidad del colesterol en sangre. Investigamos su valor, así como si se puede tratar de un caso real.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(heartdata$chol)
```

El nivel saludable de colesterol se encuentra entre 125 y 200 mg/dL. Existen casos en los que se dan niveles elevados de colesterol debidos a trastornos genéticos de origen familiar, ergo no vamos a eliminar el registro.

Por otro lado, también existen algunos outliers que nos llaman la atención respecto a la presión arterial en reposo de los pacientes, comprobamos el valor máximo de la variable.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(heartdata$trtbps)
```

Vemos que llega hasta 200. Se pueden dar casos de personas que tienen esta presión arterial en reposo cuando están sufriendo una crisis hipertensiva, usualmente acompañada de un dolor agudo en el pecho.

Una vez analizados los datos, no vemos coherente modificar o eliminar ninguno de estos valores, por tanto, procedemos a analizarlos junto al resto de registros del dataset.

******
## Análisis de los datos
******

### Comprobación de la normalidad y homogeneidad de la varianza

Estudio de la normalidad en las variables numéricas

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(ggpubr)
library(ggplot2)
library(lattice)
library(grid)
library(gridExtra)

# Varible Age
H1 <- histogram(heartdata$age)
G1 <- ggqqplot(heartdata$age)

# Variable trtbps
H2 <- histogram(heartdata$trtbps)
G2 <- ggqqplot(heartdata$trtbps)

# Variable chol
H3 <- histogram(heartdata$chol)
G3 <- ggqqplot(heartdata$chol)

# Variable thalach
H4 <- histogram(heartdata$thalach)
G4 <- ggqqplot(heartdata$thalach)

# Variable oldpeak
H5 <- histogram(heartdata$oldpeak)
G5 <- ggqqplot(heartdata$oldpeak)

# Mostramos las gráficas
grid.arrange(H1, G1)
grid.arrange(H2, G2)
grid.arrange(H3, G3)
grid.arrange(H4, G4)
grid.arrange(H5, G5)

```

Los gráficos de densidad y Q-Q nos confirman que todas las variables numéricas siguen una distribución normal. Practicamente todas las gráficas tienen una normalidad en el centro de la muestra y están un poco desviadas en los extremos, pero este dato no es suficiente para descartar la normalidad, además todas las variables tienen n>30 porque podemos asumir que siguen una distribución normal.

Estudio de la homogeneidad de las variables numéricas. 

###  Pruebas estadísticas y métodos de análisis

Para realizar este estudio aplicaremos 4 modelos: Matriz de correlación, regresión lineal, árbol de decisión y agrupación(clustering).

#### Matriz correlación

Mediante la matriz de correlación podremos observar que nivel de correlación guardan la variables entre ellas y de esta manera saber cuales son la más significativas. 

Utilizaremos el coeficiente de correlación de Pearson para ver cuán asociadas se encuentra la variables. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(GGally)
#Utilizaremos el data set original ya que todos los campos son numéricos.
heart_ori <- read.csv('../data/heart.csv')

ggcorr(heart_ori, method = c("everything", "pearson")) 

cor(heart_ori)

```

En general podemos decir que no existe muchas relación entre las variables del dataset. 

Nos centraremos en la relación entre la variable 'tarjet' (output) y el resto:

- Encontramos que slp, thalachh, restecg y cp son la que más peso tienen respecto a 'tarjet'(output) es decir, la pendiente del segmento ST en la electrocardiograma, ritmo cardíaco máximo, resultados de electrocardiograma en reposo y el tipo de dolor en el pecho. 

Todas son pruebas médicas para evaluar el estado del corazón, por lo que es lógico que tengan un peso importante, porque a través de los resultados los médicos tienen datos fiables para el diagnóstico.


#### Regresión lineal 

Mediante la regresión lineal podremos predecir si un paciente padecerá enfermedad cardíaca. 

Vamos a utilizar las variables de mayor correlación obtenidas en el apartado anterior. 

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Aplicamos el algoritmo de regresión lineal
ml.heart <- lm(output~age+sex+slp+thalachh+restecg+cp, data = heart_ori,
               family = binomial)

# Resultados
summary(ml.heart)

```
Podemos observar que la variables restecg no es significativa para el modelo de regresión ya que p-values > 0.05.

Para evaluar la eficiencia del modelo de regresión lineal vamos a utilizar la curva ROC.


```{r echo=TRUE, message=FALSE, warning=FALSE}
library(pROC)
prob = predict(ml.heart, heart_ori, type="response")
r = roc(heart_ori$output , prob, data=heart_ori)
plot(r)
r
```
El resultado del modelo es de 0.8697, por lo que la habilidad del modelo para predecir
enfermedades cardíacas es muy bueno. 

AURO > 0.8, el modelo discrimina de modo muy bueno. 


Vamos usar el modelo mencionado para realizar una predicción. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
# creamos un nuevo dataset con los datos del paciente

paciente <- data.frame(age=57,sex=1,slp=2,thalachh=162,restecg=2, cp=2)

# Algoritmo de predicción
predict(ml.heart, paciente)
```

Paciente con las siguientes patologías:

- Edad = 57

- sexo = Hombre

- Pendiente del segmento ST en la electrocardiograma = 2 - Pendiente ascendente

- Ritmo cardíaco máximo = 162 pulsaciones por minuto.

- Resultados de electrocardiograma en reposo = 2 - Normal

- Tipo de dolor en el pecho = 2 - Dolot no-anginal. 

Tiene una probabilidad de un 85% de padecer una enfermedad cardíaca según nuestro modelo de regresión lineal con una efectividad del 83%.

#### Árbol de decisión con algoritmo C50

Desordenamos la muestra mediante un algoritmo de desordenación. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(C50)
set.seed(10)
heart_r <- heartdata[sample(nrow(heartdata)),]

```

Separaremos los datos en train con una proporción  2/3 y test con una proporción de 1/3, esto es necesario para poder aplicar el algoritmo. 

Clasificaremos los datos por la variable output que se encuentra en la posición 14 del dataset. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(666)
y <- heart_r[,14] 
X <- heart_r[,1:13] 

# Creamos rangos para los subconjuntos train (2/3) y test(1/3)

indexes = sample(1:nrow(heartdata), size=floor((2/3)*nrow(heartdata)))
trainX<-X[indexes,]
trainy<-y[indexes]
testX<-X[-indexes,]
testy<-y[-indexes]
```

Aplicamos el algoritmo C50.

```{r echo=TRUE, message=FALSE, warning=FALSE}
trainy = as.factor(trainy)
mo <- C50::C5.0(trainX, trainy,rules=TRUE )
summary(mo)
```
Obtenemos la siguiente información del árbol de decisión:

El algoritmo ha obtenido 8 reglas de clasificación con un error del 11% y un acierto del 89%.

Analizaremos los mejores resultados:

class Bajo riesgo  [0.972](rule 1): Persona con tipo de dolor en el pecho Asintomático o Angina típica, con la presión arterial en resposo > 108, con colesterol en sangre > 242 y con el resultado de la prueba de esfuerzo con Defecto irreversible o  Defecto reversible tiene una probabilidad de no padecer una enfermedad cardíaca del 97%

class Alto riesgo  [0.929](Rule 5): Persona con colesterol <= 242, con depresión en el segmento ST al hacer ejercicio en relación con el reposo <= 0.4 y con el número de vasos principales del corazón  <= 0 tiene una probabilidad de padecer una enfermedad cardiaca del 93%.

Las variables más influyentes en la clasificación son: 89.00%	thall, 65.00%	caa, 61.50%	cp, 44.50%	trtbps, 38.00%	oldpeak y 30.00%	chol

Mostramos el árbol de manera gráfica. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
mo_tree <- C50::C5.0(trainX, trainy)
plot(mo_tree)
```

Vamos a obtener la precisión del modelo a través de la matriz de confusión. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
predicted_mo <- predict( mo, testX, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_mo == testy) / length(predicted_mo)))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
matriz_conf<-table(testy,Predicted=predicted_mo)
matriz_conf
```

El estudio de los datos nos muestra que 32 + 49 = 81 has sido clasificados correctamente y 7 + 13 = 20 no. Tenemos un 81% de aciertos, un porcentaje de acierto ligeramente superior al conjunto de entrenamiento.


```{r echo=TRUE, message=FALSE, warning=FALSE}
porcentaje_correct<-100 * sum(diag(matriz_conf)) / sum(matriz_conf)
print(sprintf("El %% de registros correctamente clasificados es: %.4f %%",
              porcentaje_correct))
```

El porcentaje de la matriz de confusión es igual al que hemos obtenido de las reglas de decisión. 


#### K-means

K-means es un método de agrupamiento, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano.

K-means no funciona con datos categóricos así que tendremos que hacer un subconjunto con los datos numéricos del dataset, aplicaremos algoritmos de normalizar ya que este algoritmo es muy sensible a los outliers.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
# Creamos el dataset con las variables numéricas
heart_km <- select_if(heartdata, is.numeric)

# Aplicamos algoritmo de normalizar para minimizar el efecto de los outliers.
normalizar <- function(x){
 return ((x-min(x))/(max(x)-min(x)))
}

heart_km$age <- normalizar(heart_km$age)
heart_km$trtbps   <- normalizar(heart_km$trtbps)
heart_km$thalachh <- normalizar(heart_km$thalachh)
heart_km$chol <- normalizar(heart_km$chol)
heart_km$oldpeak <- normalizar(heart_km$oldpeak)

# Elimimas caa ya que es categórica y no numérica. 
heart_km$caa <- NULL
```

Al no conocer el número de clústers optimo, probaremos con diferentes valores y métodos. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(cluster)
d <- daisy(heart_km)
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(heart_km, i)
  y_cluster     <- fit$cluster
  sk            <- silhouette(y_cluster, d)
  resultados[i] <- mean(sk[,3])
}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Numbero de clusters",
     ylab="Silueta")

```
La gráfica nos muestra que el número de cluster optimo es 2, vamos a seguir probando otros algoritmos para comprobar si realmente k=2.


Vamos aplicar el método elbow(codo). Este método utiliza los valores de la inercia obtenidos tras aplicar el K-means a diferente número de Clusters (desde 1 a N Clusters), siendo la inercia la suma de las distancias al cuadrado de cada objeto del Cluster a su centroide.
```{r echo=TRUE, message=FALSE, warning=FALSE}
resultados <- rep(0, 10)
for (i in c(2,3,4,5,6,7,8,9,10))
{
  fit           <- kmeans(heart_km, i)
  resultados[i] <- fit$tot.withinss 
}
plot(2:10,resultados[2:10],type="o",col="blue",pch=0,xlab="Numbero de clusters",
     ylab="tot.withinss")
```

La gráfica nos muestra una estabilización de la curva en k= 4.


Aplicamos el algoritmo de silueta media y Calinski-Harabasz.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(fpc)

fit_ch <- kmeansruns(heart_km, krange = 1:10, criterion = "ch")
fit_asw <- kmeansruns(heart_km, krange = 1:10, criterion = "asw")
fit_ch$bestk
fit_asw$bestk
plot(1:10,fit_ch$crit, type = "o", col="blue", pch=0, xlab="Número de clústers", 
     ylab="Criterio Silueta Media")
plot(1:10,fit_asw$crit, type = "o", col="blue", pch=0, xlab="Número de clústers"
     , ylab="Criterio Silueta Media")
```

Los dos métodos nos han dado el mismo resultado k=2.


```{r echo=TRUE, message=FALSE, warning=FALSE}
# visualización de 2 clusters
km.res2 <- kmeans(heart_km, 2, nstart = 25)
clusplot(heart_km, km.res2$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)

```
El algoritmo silhouette es 2

El algoritmo elbow(codo) es 4

El algoritmo silueta media es 2

El algoritmo de Calinski-Harabasz es 2

El resultado optimo es k=2, lo que concuerda con los datos que tenemos en el dataset, ya que puedes o no padecer enfermedad cardíaca. Esto nos confirma que la categoría de la variable target (Bajo riesgo, Alto riesgo) es correcto.  

Vamos a estudiar la precisión del modelo para k =2

```{r echo=TRUE, message=FALSE, warning=FALSE}
table(km.res2$cluster,heartdata$output)
```
```{r echo=TRUE, message=FALSE, warning=FALSE}
100*((99+119)/(218+38+45))
```

Tenemos un total de instancias correctamente clasificadas de 218, el total de casos con error de clasificación son 85, por lo que podemos calcular:
exactitud = 218/(218+83) = 0,7242, con lo que el modelo para k=2 tienen una exactitud del 72%.

******
## Conclusión
******

Inicialmente hemos realizado un proceso de limpieza y preprocesado de los datos del dataset que estamos estudiando, el cual contiene datos reales sobre pruebas médicas para el diagnóstico de enfermedades cardíacas. En este proceso hemos cambiado los nombres a las variables y hemos estudiado los valores extremos(Outliers).

Seguidamente hicimos un análisis exploratorio del conjunto de variables que componen el dataset para tener una primera aproximación de su comportamiento, y así poder hacernos una idea de como se comportarán al aplicar algoritmos estadísticos. 

Aplicamos un algoritmo de regresión lineal que es una buena manera de conocer la relación que existe entre las variables del modelo, seguidamente aplicamos el algoritmo supervisado de arbole de decisión que nos sirve para sabes los diferentes casos (reglas de comportamiento) de la variable objetivo (Output)

Para finalizar, aplicamos un algoritmo no supervisado de clasificación (K-means) el cual nos dió como resultado 2 clusters tal y como se agrupa la variable objetivo(output) (Bajo riesgo, Alto riesgo).

Cada uno de los pasos realizados en este estudios nos han ayudado a tener una profunda compresión de los datos que conforman el dataset. Por lo tanto, podemos afirmar, que hemos podido responder el problema de inicio, el cual consistía en poder responder mediante el análisis de datos si se podía diagnosticar una enfermedad cardíaca. 


******
## Recursos
******
● Dr.Luis Azcona - El electrocardiograma.  https://www.fbbva.es/microsites/salud_cardio/mult/fbbva_libroCorazon_cap4.pdf

● Healthline.com - Thallium Stress Test. https://www.healthline.com/health/thallium-stress-test

******
## Contribuciones/Firma integrantes
******

```{r echo=TRUE, message=FALSE, warning=FALSE}
integrantes <- data.frame(Contribuciones=
   c("Investigación previa","Redacción de las respuestas","Desarrollo código"), 
   Firma=c("Jose Antonio Jara Pérez / Óscar López Montero",
           "Jose Antonio Jara Pérez / Óscar López Montero",
           "Jose Antonio Jara Pérez / Óscar López Montero"))

integrantes
```

